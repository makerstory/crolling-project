import requests
from bs4 import BeautifulSoup
import time
import json
import os

# --- [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° í‚¤ì›Œë“œ ---
KEYWORDS = ["ì±„ìš©", "ëª¨ì§‘", "ê³µê³ ", "ëŠ˜ë´„"]
DATA_FILE = "sent_posts.json"

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° ì˜ˆì™¸ì²˜ë¦¬
BOT_TOKEN = os.environ.get('BOT_TOKEN')
try:
    CHAT_ID = int(os.environ.get('CHAT_ID'))
except (TypeError, ValueError):
    print("ERROR: CHAT_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤.")
    CHAT_ID = 0

# --- [ê³µí†µ í•¨ìˆ˜] ì•Œë¦¼ ë° ë°ì´í„° ê´€ë¦¬ ---
def send_telegram_message(text):
    if not BOT_TOKEN or not CHAT_ID:
        return
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {"chat_id": CHAT_ID, "text": text}
    try:
        requests.post(url, json=payload)
    except Exception as e:
        print(f"ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def load_sent_posts():
    if not os.path.exists(DATA_FILE):
        return []
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_sent_posts(posts):
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(posts, f, ensure_ascii=False, indent=4)

# ============================================================================
# [ì‚¬ì´íŠ¸ë³„ íŒŒì‹± í•¨ìˆ˜ êµ¬ê°„] 
# ê° í•¨ìˆ˜ëŠ” soup ê°ì²´ë¥¼ ë°›ì•„ -> [{'id':, 'title':, 'link':, 'author':}, ...] ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
# ============================================================================

def parse_site_A(soup):
    """
    1ë²ˆ ì‚¬ì´íŠ¸: ê²½ì‚°êµìœ¡ì²­
    êµ¬ì¡°: .BD_list í…Œì´ë¸” í˜•íƒœ
    """
    results = []
    base_url = "https://www.gbe.kr/gs/na/ntt/selectNttInfo.do?mi=19265&bbsId=2577&nttSn="
    
    rows = soup.select(".BD_list tr")
    for row in rows:
        tds = row.find_all("td")
        if len(tds) < 3: continue
        
        title_tag = tds[1].find("a")
        if not title_tag: continue
        
        title = title_tag.get_text(strip=True)
        author = tds[2].get_text(strip=True)
        
        # data-idë¥¼ ì´ìš©í•œ ë§í¬ ì¬êµ¬ì„±
        data_id = title_tag.get('data-id')
        if not data_id: continue
        
        link = f"{base_url}{data_id}"
        
        results.append({
            'id': link,       # ê³ ìœ  ì‹ë³„ì (ë³´í†µ ë§í¬ ì‚¬ìš©)
            'title': title,
            'link': link,
            'author': author
        })
    return results

def parse_site_B(soup):
    """
    2ë²ˆ ì‚¬ì´íŠ¸: ì²­ë„êµìœ¡ì²­
    êµ¬ì¡°: .BD_list í…Œì´ë¸” í˜•íƒœ
    """
    results = []
    base_url = "https://www.gbe.kr/cd/na/ntt/selectNttInfo.do?mi=10467&bbsId=3251&nttSn="
    
    rows = soup.select(".BD_list tr")
    for row in rows:
        tds = row.find_all("td")
        if len(tds) < 3: continue
        
        title_tag = tds[1].find("a")
        if not title_tag: continue
        
        title = title_tag.get_text(strip=True)
        author = tds[2].get_text(strip=True)
        
        # data-idë¥¼ ì´ìš©í•œ ë§í¬ ì¬êµ¬ì„±
        data_id = title_tag.get('data-id')
        if not data_id: continue
        
        link = f"{base_url}{data_id}"
        
        results.append({
            'id': link,       # ê³ ìœ  ì‹ë³„ì (ë³´í†µ ë§í¬ ì‚¬ìš©)
            'title': title,
            'link': link,
            'author': author
        })
    return results

def parse_site_C(soup):
    """
    3ë²ˆ ì‚¬ì´íŠ¸: ì˜ì²œêµìœ¡ì²­
    êµ¬ì¡°: .BD_list í…Œì´ë¸” í˜•íƒœ
    """
    results = []
    base_url = "https://www.gbe.kr/yc/na/ntt/selectNttInfo.do?mi=4403&bbsId=2078&nttSn="
    
    rows = soup.select(".BD_list tr")
    for row in rows:
        tds = row.find_all("td")
        if len(tds) < 3: continue
        
        title_tag = tds[1].find("a")
        if not title_tag: continue
        
        title = title_tag.get_text(strip=True)
        author = tds[2].get_text(strip=True)
        
        # data-idë¥¼ ì´ìš©í•œ ë§í¬ ì¬êµ¬ì„±
        data_id = title_tag.get('data-id')
        if not data_id: continue
        
        link = f"{base_url}{data_id}"
        
        results.append({
            'id': link,       # ê³ ìœ  ì‹ë³„ì (ë³´í†µ ë§í¬ ì‚¬ìš©)
            'title': title,
            'link': link,
            'author': author
        })
    return results

def parse_site_D(soup):
    """
    4ë²ˆ ì‚¬ì´íŠ¸: ê²½ì£¼êµìœ¡ì²­
    êµ¬ì¡°: .BD_list í…Œì´ë¸” í˜•íƒœ
    """
    results = []
    base_url = "https://www.gbe.kr/gj/na/ntt/selectNttInfo.do?mi=11638&bbsId=1583&nttSn="
    
    rows = soup.select(".BD_list tr")
    for row in rows:
        tds = row.find_all("td")
        if len(tds) < 3: continue
        
        title_tag = tds[1].find("a")
        if not title_tag: continue
        
        title = title_tag.get_text(strip=True)
        author = tds[2].get_text(strip=True)
        
        # data-idë¥¼ ì´ìš©í•œ ë§í¬ ì¬êµ¬ì„±
        data_id = title_tag.get('data-id')
        if not data_id: continue
        
        link = f"{base_url}{data_id}"
        
        results.append({
            'id': link,       # ê³ ìœ  ì‹ë³„ì (ë³´í†µ ë§í¬ ì‚¬ìš©)
            'title': title,
            'link': link,
            'author': author
        })
    return results

# ============================================================================
# [ë©”ì¸ ì‹¤í–‰ ë¡œì§]
# ============================================================================

def run_crawlers():
    print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] í¬ë¡¤ë§ ì‹œì‘...")
    sent_posts = load_sent_posts()
    new_posts_found = False
    
    # 1. í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸ ì •ì˜
    # (URL, íŒŒì‹±í•¨ìˆ˜ì´ë¦„, ì‚¬ì´íŠ¸ë³„ì¹­)
    TARGETS = [
        (
            "https://www.gbe.kr/gs/na/ntt/selectNttList.do?mi=19265&bbsId=2577", 
            parse_site_A,
            "ê²½ì‚°êµìœ¡ì²­"
        ),
        (
            "https://www.gbe.kr/cd/na/ntt/selectNttList.do?mi=10467&bbsId=3251",
            parse_site_B,
            "ì²­ë„êµìœ¡ì²­"
        ),
        (
            "https://www.gbe.kr/yc/na/ntt/selectNttList.do?mi=4403&bbsId=2078",
            parse_site_C,
            "ì˜ì²œêµìœ¡ì²­"
        ),
        (
            "https://www.gbe.kr/gj/na/ntt/selectNttList.do?mi=11638&bbsId=1583",
            parse_site_D,
            "ê²½ì£¼êµìœ¡ì²­"
        ),
        # í•„ìš”í•˜ë©´ ë” ì¶”ê°€ ê°€ëŠ¥
    ]

    for url, parser_func, site_name in TARGETS:
        print(f"  Target: {site_name} í™•ì¸ ì¤‘...")
        try:
            response = requests.get(url, timeout=10) # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
            response.encoding = 'utf-8' # í•„ìš”ì‹œ 'euc-kr' ë“±ìœ¼ë¡œ ë³€ê²½
            
            if response.status_code != 200:
                print(f"  [Error] {site_name} ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
                continue
                
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í•´ë‹¹ ì‚¬ì´íŠ¸ ì „ìš© íŒŒì„œ ì‹¤í–‰
            posts = parser_func(soup) 

            for post in posts:
                # í‚¤ì›Œë“œ í•„í„°ë§
                if any(k in post['title'] for k in KEYWORDS):
                    # ì¤‘ë³µ í•„í„°ë§
                    if post['id'] not in sent_posts:
                        msg = (
                            f"ğŸ”” [{site_name} ìƒˆ ê³µê³ ]\n"
                            f"*ì œëª©*: {post['title']}\n"
                            f"*ì‘ì„±ì*: {post['author']}\n"
                            f"*ë§í¬*: {post['link']}"
                        )
                        print(msg)
                        send_telegram_message(msg)
                        
                        sent_posts.append(post['id'])
                        new_posts_found = True
                        
        except Exception as e:
            print(f"  [Error] {site_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ íŒŒì¼ ì €ì¥
    if new_posts_found:
        save_sent_posts(sent_posts)
    
    print("í¬ë¡¤ë§ ì¢…ë£Œ.")

if __name__ == "__main__":
    run_crawlers()
